{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('Tweets_cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 12)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>text2</th>\n",
       "      <th>text3</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_remove_stopwords</th>\n",
       "      <th>length_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66659</th>\n",
       "      <td>0</td>\n",
       "      <td>1691902118</td>\n",
       "      <td>Sun May 03 18:51:20 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>cimtrbl2</td>\n",
       "      <td>still sick n very bored lol any suggestions on...</td>\n",
       "      <td>still sick n very bored lol any suggestions on...</td>\n",
       "      <td>still sick n very any on what on</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['still', 'sick', 'n', 'very', 'any', 'on', 'w...</td>\n",
       "      <td>['still', 'sick', 'n']</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042629</th>\n",
       "      <td>4</td>\n",
       "      <td>1957227096</td>\n",
       "      <td>Thu May 28 23:52:59 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>DarianLovesYou</td>\n",
       "      <td>ii think i'm overdosing on sprite lawl</td>\n",
       "      <td>ii think im overdosing on sprite lawl</td>\n",
       "      <td>think on sprite</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['think', 'on', 'sprite']</td>\n",
       "      <td>['think', 'sprite']</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1380918</th>\n",
       "      <td>4</td>\n",
       "      <td>2052294910</td>\n",
       "      <td>Sat Jun 06 00:15:19 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>jamiehitchcock</td>\n",
       "      <td>@dmcclure really sorry to hear that. hope he w...</td>\n",
       "      <td>really sorry to hear that hope he will feel be...</td>\n",
       "      <td>really sorry to hear that hope he will feel be...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['really', 'sorry', 'to', 'hear', 'that', 'hop...</td>\n",
       "      <td>['really', 'sorry', 'hear', 'hope', 'feel', 'b...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101149</th>\n",
       "      <td>0</td>\n",
       "      <td>1794315683</td>\n",
       "      <td>Thu May 14 05:31:15 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>SoozyJ</td>\n",
       "      <td>@themakelounge the link doesn't work  i want t...</td>\n",
       "      <td>the link doesnt work i want to decorate cupcak...</td>\n",
       "      <td>the link doesnt work i want to decorate proper...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['the', 'link', 'doesnt', 'work', 'i', 'want',...</td>\n",
       "      <td>['link', 'doesnt', 'work', 'want', 'decorate',...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019882</th>\n",
       "      <td>4</td>\n",
       "      <td>1882268537</td>\n",
       "      <td>Fri May 22 06:33:32 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Scalper68</td>\n",
       "      <td>@g2trading ty</td>\n",
       "      <td>ty</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347261</th>\n",
       "      <td>4</td>\n",
       "      <td>2044500919</td>\n",
       "      <td>Fri Jun 05 09:27:03 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>chantellmarie</td>\n",
       "      <td>@cacaubrazil o ok. i got it! spanish does it k...</td>\n",
       "      <td>o ok i got it spanish does it kinda the same a...</td>\n",
       "      <td>o i got it does it the same as for and os for ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['o', 'i', 'got', 'it', 'does', 'it', 'the', '...</td>\n",
       "      <td>['got', 'os', 'day', 'going']</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1553382</th>\n",
       "      <td>4</td>\n",
       "      <td>2184624413</td>\n",
       "      <td>Mon Jun 15 16:10:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>xocarissajonas</td>\n",
       "      <td>http://www.myspace.com/pop2kpromotions &amp;lt;-- ...</td>\n",
       "      <td>lt wont stop tweeting about it untill ig et ad...</td>\n",
       "      <td>wont stop about it untill from its brand new</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['wont', 'stop', 'about', 'it', 'untill', 'fro...</td>\n",
       "      <td>['wont', 'stop', 'untill', 'brand', 'new']</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370333</th>\n",
       "      <td>4</td>\n",
       "      <td>2050994869</td>\n",
       "      <td>Fri Jun 05 19:40:12 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ahhingtina</td>\n",
       "      <td>@agelufailau hey it's hella funny! your new pi...</td>\n",
       "      <td>hey its hella funny your new pic hella looks l...</td>\n",
       "      <td>hey its funny your new pic like like always w ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['hey', 'its', 'funny', 'your', 'new', 'pic', ...</td>\n",
       "      <td>['hey', 'funny', 'new', 'pic', 'like', 'like',...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69008</th>\n",
       "      <td>0</td>\n",
       "      <td>1693111418</td>\n",
       "      <td>Sun May 03 21:35:40 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mosesmonster</td>\n",
       "      <td>@staceybeeeee north carolina!  you moving to s...</td>\n",
       "      <td>north carolina you moving to santa monica this...</td>\n",
       "      <td>north you moving to this fall</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['north', 'you', 'moving', 'to', 'this', 'fall']</td>\n",
       "      <td>['north', 'moving', 'fall']</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817657</th>\n",
       "      <td>4</td>\n",
       "      <td>1551843454</td>\n",
       "      <td>Sat Apr 18 10:07:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>firsttiger</td>\n",
       "      <td>@kaznakamura henry tsang has joined twitter as...</td>\n",
       "      <td>henry tsang has joined twitter as should show ...</td>\n",
       "      <td>henry twitter as should show him some tweet love</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['henry', 'twitter', 'as', 'should', 'show', '...</td>\n",
       "      <td>['henry', 'twitter', 'show', 'tweet', 'love']</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>320000 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target          id                          date      flag  \\\n",
       "66659         0  1691902118  Sun May 03 18:51:20 PDT 2009  NO_QUERY   \n",
       "1042629       4  1957227096  Thu May 28 23:52:59 PDT 2009  NO_QUERY   \n",
       "1380918       4  2052294910  Sat Jun 06 00:15:19 PDT 2009  NO_QUERY   \n",
       "101149        0  1794315683  Thu May 14 05:31:15 PDT 2009  NO_QUERY   \n",
       "1019882       4  1882268537  Fri May 22 06:33:32 PDT 2009  NO_QUERY   \n",
       "...         ...         ...                           ...       ...   \n",
       "1347261       4  2044500919  Fri Jun 05 09:27:03 PDT 2009  NO_QUERY   \n",
       "1553382       4  2184624413  Mon Jun 15 16:10:49 PDT 2009  NO_QUERY   \n",
       "1370333       4  2050994869  Fri Jun 05 19:40:12 PDT 2009  NO_QUERY   \n",
       "69008         0  1693111418  Sun May 03 21:35:40 PDT 2009  NO_QUERY   \n",
       "817657        4  1551843454  Sat Apr 18 10:07:00 PDT 2009  NO_QUERY   \n",
       "\n",
       "                   user                                               text  \\\n",
       "66659          cimtrbl2  still sick n very bored lol any suggestions on...   \n",
       "1042629  DarianLovesYou            ii think i'm overdosing on sprite lawl    \n",
       "1380918  jamiehitchcock  @dmcclure really sorry to hear that. hope he w...   \n",
       "101149           SoozyJ  @themakelounge the link doesn't work  i want t...   \n",
       "1019882       Scalper68                                     @g2trading ty    \n",
       "...                 ...                                                ...   \n",
       "1347261   chantellmarie  @cacaubrazil o ok. i got it! spanish does it k...   \n",
       "1553382  xocarissajonas  http://www.myspace.com/pop2kpromotions &lt;-- ...   \n",
       "1370333      ahhingtina  @agelufailau hey it's hella funny! your new pi...   \n",
       "69008      mosesmonster  @staceybeeeee north carolina!  you moving to s...   \n",
       "817657       firsttiger  @kaznakamura henry tsang has joined twitter as...   \n",
       "\n",
       "                                                     text2  \\\n",
       "66659    still sick n very bored lol any suggestions on...   \n",
       "1042629              ii think im overdosing on sprite lawl   \n",
       "1380918  really sorry to hear that hope he will feel be...   \n",
       "101149   the link doesnt work i want to decorate cupcak...   \n",
       "1019882                                                 ty   \n",
       "...                                                    ...   \n",
       "1347261  o ok i got it spanish does it kinda the same a...   \n",
       "1553382  lt wont stop tweeting about it untill ig et ad...   \n",
       "1370333  hey its hella funny your new pic hella looks l...   \n",
       "69008    north carolina you moving to santa monica this...   \n",
       "817657   henry tsang has joined twitter as should show ...   \n",
       "\n",
       "                                                     text3 hashtags  \\\n",
       "66659                     still sick n very any on what on      NaN   \n",
       "1042629                                    think on sprite      NaN   \n",
       "1380918  really sorry to hear that hope he will feel be...      NaN   \n",
       "101149   the link doesnt work i want to decorate proper...      NaN   \n",
       "1019882                                                NaN      NaN   \n",
       "...                                                    ...      ...   \n",
       "1347261  o i got it does it the same as for and os for ...      NaN   \n",
       "1553382       wont stop about it untill from its brand new      NaN   \n",
       "1370333  hey its funny your new pic like like always w ...      NaN   \n",
       "69008                        north you moving to this fall      NaN   \n",
       "817657    henry twitter as should show him some tweet love      NaN   \n",
       "\n",
       "                                                    tokens  \\\n",
       "66659    ['still', 'sick', 'n', 'very', 'any', 'on', 'w...   \n",
       "1042629                          ['think', 'on', 'sprite']   \n",
       "1380918  ['really', 'sorry', 'to', 'hear', 'that', 'hop...   \n",
       "101149   ['the', 'link', 'doesnt', 'work', 'i', 'want',...   \n",
       "1019882                                                 []   \n",
       "...                                                    ...   \n",
       "1347261  ['o', 'i', 'got', 'it', 'does', 'it', 'the', '...   \n",
       "1553382  ['wont', 'stop', 'about', 'it', 'untill', 'fro...   \n",
       "1370333  ['hey', 'its', 'funny', 'your', 'new', 'pic', ...   \n",
       "69008     ['north', 'you', 'moving', 'to', 'this', 'fall']   \n",
       "817657   ['henry', 'twitter', 'as', 'should', 'show', '...   \n",
       "\n",
       "                                   tokens_remove_stopwords  length_1  \n",
       "66659                               ['still', 'sick', 'n']         3  \n",
       "1042629                                ['think', 'sprite']         2  \n",
       "1380918  ['really', 'sorry', 'hear', 'hope', 'feel', 'b...         7  \n",
       "101149   ['link', 'doesnt', 'work', 'want', 'decorate',...         7  \n",
       "1019882                                                 []         0  \n",
       "...                                                    ...       ...  \n",
       "1347261                      ['got', 'os', 'day', 'going']         4  \n",
       "1553382         ['wont', 'stop', 'untill', 'brand', 'new']         5  \n",
       "1370333  ['hey', 'funny', 'new', 'pic', 'like', 'like',...         9  \n",
       "69008                          ['north', 'moving', 'fall']         3  \n",
       "817657       ['henry', 'twitter', 'show', 'tweet', 'love']         5  \n",
       "\n",
       "[320000 rows x 12 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.sample(frac = 0.2)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "\n",
    "# parameter setting\n",
    "n_samples = data.shape[0]\n",
    "n_features = 1000\n",
    "n_components = 20\n",
    "n_top_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(max_df=0.90, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(data['text3'].values.astype('U'))\n",
    "feature_name = tf_vectorizer.get_feature_names()\n",
    "index_dict = dict(enumerate(feature_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<320000x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1081249 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'able', 1: 'absolutely', 2: 'account', 3: 'ache', 4: 'actually', 5: 'add', 6: 'added', 7: 'adorable', 8: 'advice', 9: 'afford', 10: 'afraid', 11: 'afternoon', 12: 'age', 13: 'ago', 14: 'agree', 15: 'ah', 16: 'ahead', 17: 'aint', 18: 'air', 19: 'airport', 20: 'album', 21: 'alive', 22: 'alright', 23: 'amazing', 24: 'angry', 25: 'annoying', 26: 'answer', 27: 'anyways', 28: 'apparently', 29: 'apple', 30: 'appreciate', 31: 'area', 32: 'arent', 33: 'arm', 34: 'art', 35: 'article', 36: 'ask', 37: 'asleep', 38: 'ass', 39: 'ate', 40: 'august', 41: 'available', 42: 'aw', 43: 'awake', 44: 'away', 45: 'awesome', 46: 'awful', 47: 'babe', 48: 'baby', 49: 'background', 50: 'bad', 51: 'badly', 52: 'bag', 53: 'ball', 54: 'band', 55: 'bank', 56: 'bar', 57: 'barely', 58: 'bath', 59: 'battery', 60: 'beach', 61: 'beat', 62: 'beautiful', 63: 'bed', 64: 'beer', 65: 'believe', 66: 'best', 67: 'bet', 68: 'better', 69: 'big', 70: 'bike', 71: 'bird', 72: 'birthday', 73: 'bit', 74: 'bitch', 75: 'black', 76: 'blackberry', 77: 'blah', 78: 'blast', 79: 'bless', 80: 'block', 81: 'blood', 82: 'bloody', 83: 'blue', 84: 'board', 85: 'body', 86: 'boo', 87: 'book', 88: 'boring', 89: 'boss', 90: 'bottle', 91: 'bought', 92: 'bout', 93: 'box', 94: 'boy', 95: 'brain', 96: 'brazil', 97: 'bread', 98: 'break', 99: 'breakfast', 100: 'breaking', 101: 'bright', 102: 'brilliant', 103: 'bring', 104: 'broke', 105: 'broken', 106: 'brother', 107: 'brought', 108: 'buddy', 109: 'bug', 110: 'bummed', 111: 'bummer', 112: 'bunch', 113: 'burn', 114: 'burnt', 115: 'bus', 116: 'business', 117: 'busy', 118: 'butt', 119: 'button', 120: 'buy', 121: 'bye', 122: 'cake', 123: 'calling', 124: 'came', 125: 'camera', 126: 'camp', 127: 'canada', 128: 'car', 129: 'card', 130: 'care', 131: 'case', 132: 'cat', 133: 'catch', 134: 'catching', 135: 'caught', 136: 'cause', 137: 'cell', 138: 'chance', 139: 'change', 140: 'channel', 141: 'chat', 142: 'check', 143: 'checked', 144: 'cheer', 145: 'cheese', 146: 'chicken', 147: 'chill', 148: 'chocolate', 149: 'choice', 150: 'church', 151: 'city', 152: 'class', 153: 'classes', 154: 'clean', 155: 'cleaning', 156: 'close', 157: 'closed', 158: 'clothes', 159: 'club', 160: 'code', 161: 'coffee', 162: 'cold', 163: 'college', 164: 'come', 165: 'comes', 166: 'coming', 167: 'comment', 168: 'company', 169: 'complete', 170: 'completely', 171: 'computer', 172: 'concert', 173: 'confused', 174: 'cook', 175: 'cooking', 176: 'cool', 177: 'copy', 178: 'cos', 179: 'couch', 180: 'cough', 181: 'count', 182: 'country', 183: 'couple', 184: 'course', 185: 'cousin', 186: 'cover', 187: 'coz', 188: 'crap', 189: 'crazy', 190: 'cream', 191: 'cried', 192: 'crying', 193: 'cup', 194: 'currently', 195: 'cut', 196: 'cute', 197: 'da', 198: 'dad', 199: 'daddy', 200: 'damn', 201: 'dance', 202: 'dancing', 203: 'dang', 204: 'dark', 205: 'darn', 206: 'date', 207: 'daughter', 208: 'day', 209: 'days', 210: 'dead', 211: 'deal', 212: 'dear', 213: 'death', 214: 'decided', 215: 'definitely', 216: 'delicious', 217: 'demi', 218: 'dentist', 219: 'depressed', 220: 'depressing', 221: 'deserve', 222: 'design', 223: 'did', 224: 'didnt', 225: 'die', 226: 'different', 227: 'difficult', 228: 'dinner', 229: 'disappointed', 230: 'doctor', 231: 'does', 232: 'doesnt', 233: 'dog', 234: 'dogs', 235: 'doing', 236: 'dont', 237: 'door', 238: 'double', 239: 'doubt', 240: 'dream', 241: 'dress', 242: 'drink', 243: 'drinking', 244: 'drive', 245: 'driving', 246: 'drop', 247: 'drunk', 248: 'dude', 249: 'dumb', 250: 'dying', 251: 'ear', 252: 'early', 253: 'easier', 254: 'easy', 255: 'eat', 256: 'eating', 257: 'eh', 258: 'em', 259: 'end', 260: 'ended', 261: 'ending', 262: 'energy', 263: 'enjoy', 264: 'enjoying', 265: 'entire', 266: 'epic', 267: 'episode', 268: 'especially', 269: 'evening', 270: 'event', 271: 'everybody', 272: 'everyday', 273: 'exactly', 274: 'exam', 275: 'excellent', 276: 'excited', 277: 'exciting', 278: 'exhausted', 279: 'expensive', 280: 'experience', 281: 'extra', 282: 'eye', 283: 'fabulous', 284: 'face', 285: 'fact', 286: 'fail', 287: 'fair', 288: 'fake', 289: 'fall', 290: 'falling', 291: 'fam', 292: 'family', 293: 'fan', 294: 'fantastic', 295: 'far', 296: 'fast', 297: 'fat', 298: 'favorite', 299: 'feed', 300: 'feel', 301: 'feeling', 302: 'fell', 303: 'felt', 304: 'festival', 305: 'fever', 306: 'fight', 307: 'figure', 308: 'figured', 309: 'film', 310: 'final', 311: 'finale', 312: 'finally', 313: 'finding', 314: 'fine', 315: 'finger', 316: 'finish', 317: 'finished', 318: 'finishing', 319: 'fish', 320: 'fit', 321: 'fix', 322: 'fixed', 323: 'flat', 324: 'flight', 325: 'floor', 326: 'flu', 327: 'fly', 328: 'flying', 329: 'follow', 330: 'follower', 331: 'following', 332: 'food', 333: 'foot', 334: 'football', 335: 'forever', 336: 'forget', 337: 'forgot', 338: 'forward', 339: 'free', 340: 'fresh', 341: 'friend', 342: 'fun', 343: 'funny', 344: 'future', 345: 'game', 346: 'garden', 347: 'gave', 348: 'gay', 349: 'getting', 350: 'gift', 351: 'gig', 352: 'girl', 353: 'giving', 354: 'glad', 355: 'glass', 356: 'god', 357: 'goes', 358: 'going', 359: 'gone', 360: 'good', 361: 'goodness', 362: 'gorgeous', 363: 'gosh', 364: 'got', 365: 'gotten', 366: 'grad', 367: 'graduation', 368: 'great', 369: 'green', 370: 'group', 371: 'guess', 372: 'guitar', 373: 'guy', 374: 'gym', 375: 'ha', 376: 'hair', 377: 'half', 378: 'hand', 379: 'hanging', 380: 'happen', 381: 'happening', 382: 'happy', 383: 'hard', 384: 'harry', 385: 'hate', 386: 'havent', 387: 'head', 388: 'headache', 389: 'headed', 390: 'heading', 391: 'hear', 392: 'heart', 393: 'heat', 394: 'hell', 395: 'hello', 396: 'help', 397: 'helping', 398: 'hey', 399: 'hi', 400: 'high', 401: 'hilarious', 402: 'history', 403: 'hit', 404: 'hold', 405: 'holiday', 406: 'holy', 407: 'home', 408: 'homework', 409: 'honey', 410: 'hope', 411: 'hopefully', 412: 'horrible', 413: 'hospital', 414: 'hot', 415: 'hotel', 416: 'hour', 417: 'house', 418: 'hubby', 419: 'hug', 420: 'huge', 421: 'huh', 422: 'hungry', 423: 'hurt', 424: 'hurting', 425: 'husband', 426: 'ice', 427: 'id', 428: 'idea', 429: 'ill', 430: 'imagine', 431: 'important', 432: 'inside', 433: 'instead', 434: 'interested', 435: 'interesting', 436: 'interview', 437: 'issue', 438: 'jealous', 439: 'job', 440: 'joe', 441: 'join', 442: 'joke', 443: 'joy', 444: 'june', 445: 'just', 446: 'keeping', 447: 'kept', 448: 'kick', 449: 'kill', 450: 'killing', 451: 'kind', 452: 'kiss', 453: 'kitchen', 454: 'kitty', 455: 'knew', 456: 'know', 457: 'la', 458: 'ladies', 459: 'lady', 460: 'lake', 461: 'lame', 462: 'late', 463: 'lately', 464: 'later', 465: 'laugh', 466: 'laughing', 467: 'laundry', 468: 'lay', 469: 'laying', 470: 'lazy', 471: 'learn', 472: 'learning', 473: 'leave', 474: 'leaving', 475: 'left', 476: 'leg', 477: 'lesson', 478: 'let', 479: 'library', 480: 'lie', 481: 'life', 482: 'light', 483: 'like', 484: 'line', 485: 'link', 486: 'list', 487: 'listen', 488: 'listening', 489: 'little', 490: 'live', 491: 'lived', 492: 'living', 493: 'load', 494: 'local', 495: 'lonely', 496: 'long', 497: 'longer', 498: 'look', 499: 'looking', 500: 'lose', 501: 'losing', 502: 'loss', 503: 'lost', 504: 'lot', 505: 'lots', 506: 'loud', 507: 'love', 508: 'lovely', 509: 'loving', 510: 'low', 511: 'luck', 512: 'lucky', 513: 'lunch', 514: 'ma', 515: 'mac', 516: 'machine', 517: 'mad', 518: 'magic', 519: 'mail', 520: 'major', 521: 'make', 522: 'making', 523: 'mall', 524: 'man', 525: 'marathon', 526: 'mark', 527: 'match', 528: 'mate', 529: 'math', 530: 'matter', 531: 'maybe', 532: 'mean', 533: 'meant', 534: 'media', 535: 'meet', 536: 'meeting', 537: 'men', 538: 'mention', 539: 'mess', 540: 'message', 541: 'met', 542: 'middle', 543: 'midnight', 544: 'milk', 545: 'min', 546: 'mind', 547: 'minute', 548: 'miss', 549: 'missing', 550: 'mobile', 551: 'moment', 552: 'mommy', 553: 'money', 554: 'month', 555: 'mood', 556: 'moon', 557: 'morning', 558: 'mother', 559: 'mouth', 560: 'movie', 561: 'moving', 562: 'mum', 563: 'music', 564: 'na', 565: 'nan', 566: 'nap', 567: 'near', 568: 'nearly', 569: 'neck', 570: 'need', 571: 'needs', 572: 'nervous', 573: 'new', 574: 'news', 575: 'nice', 576: 'nick', 577: 'night', 578: 'nights', 579: 'nope', 580: 'normal', 581: 'north', 582: 'nose', 583: 'note', 584: 'number', 585: 'office', 586: 'official', 587: 'officially', 588: 'oh', 589: 'old', 590: 'open', 591: 'order', 592: 'ouch', 593: 'outside', 594: 'pack', 595: 'page', 596: 'pain', 597: 'pants', 598: 'paper', 599: 'park', 600: 'party', 601: 'pass', 602: 'past', 603: 'pay', 604: 'peace', 605: 'people', 606: 'perfect', 607: 'person', 608: 'personal', 609: 'phone', 610: 'photo', 611: 'pic', 612: 'pick', 613: 'picked', 614: 'picture', 615: 'piece', 616: 'pink', 617: 'pizza', 618: 'place', 619: 'plan', 620: 'plane', 621: 'play', 622: 'plus', 623: 'point', 624: 'pool', 625: 'poor', 626: 'pop', 627: 'positive', 628: 'possible', 629: 'possibly', 630: 'post', 631: 'posted', 632: 'posting', 633: 'power', 634: 'practice', 635: 'pray', 636: 'pretty', 637: 'prob', 638: 'probably', 639: 'problem', 640: 'productive', 641: 'profile', 642: 'project', 643: 'promise', 644: 'puppy', 645: 'question', 646: 'quick', 647: 'quiet', 648: 'quite', 649: 'quot', 650: 'race', 651: 'radio', 652: 'rain', 653: 'rainy', 654: 'ran', 655: 'random', 656: 'read', 657: 'reading', 658: 'ready', 659: 'real', 660: 'reality', 661: 'realize', 662: 'really', 663: 'reason', 664: 'record', 665: 'red', 666: 'relax', 667: 'release', 668: 'remember', 669: 'reply', 670: 'rest', 671: 'return', 672: 'review', 673: 'revision', 674: 'ride', 675: 'right', 676: 'ring', 677: 'rip', 678: 'road', 679: 'rob', 680: 'rock', 681: 'roll', 682: 'room', 683: 'rough', 684: 'round', 685: 'row', 686: 'ruined', 687: 'run', 688: 'running', 689: 'sa', 690: 'sad', 691: 'sadly', 692: 'safe', 693: 'said', 694: 'sale', 695: 'san', 696: 'sat', 697: 'save', 698: 'saw', 699: 'say', 700: 'saying', 701: 'scary', 702: 'school', 703: 'science', 704: 'screen', 705: 'search', 706: 'season', 707: 'second', 708: 'secret', 709: 'seeing', 710: 'seen', 711: 'self', 712: 'sell', 713: 'send', 714: 'sending', 715: 'sense', 716: 'sent', 717: 'series', 718: 'seriously', 719: 'service', 720: 'session', 721: 'set', 722: 'sex', 723: 'sexy', 724: 'shall', 725: 'shame', 726: 'share', 727: 'shift', 728: 'shirt', 729: 'shoot', 730: 'shop', 731: 'shopping', 732: 'short', 733: 'shot', 734: 'shouldnt', 735: 'shout', 736: 'shower', 737: 'showing', 738: 'shut', 739: 'sick', 740: 'sigh', 741: 'sign', 742: 'silly', 743: 'simple', 744: 'sing', 745: 'singing', 746: 'single', 747: 'sir', 748: 'sis', 749: 'sister', 750: 'sit', 751: 'site', 752: 'sitting', 753: 'sky', 754: 'sleep', 755: 'sleeping', 756: 'sleepy', 757: 'slept', 758: 'slow', 759: 'small', 760: 'smell', 761: 'smile', 762: 'social', 763: 'sold', 764: 'somebody', 765: 'son', 766: 'song', 767: 'soon', 768: 'sore', 769: 'sorry', 770: 'sort', 771: 'sound', 772: 'soup', 773: 'south', 774: 'space', 775: 'speak', 776: 'special', 777: 'spend', 778: 'spending', 779: 'spent', 780: 'spot', 781: 'st', 782: 'stand', 783: 'star', 784: 'start', 785: 'starting', 786: 'state', 787: 'station', 788: 'stay', 789: 'stayed', 790: 'step', 791: 'stick', 792: 'stomach', 793: 'stop', 794: 'stopped', 795: 'store', 796: 'storm', 797: 'story', 798: 'straight', 799: 'street', 800: 'stress', 801: 'strong', 802: 'stuck', 803: 'studio', 804: 'study', 805: 'stuff', 806: 'stupid', 807: 'style', 808: 'suck', 809: 'sum', 810: 'summer', 811: 'sun', 812: 'sunny', 813: 'sunshine', 814: 'super', 815: 'support', 816: 'suppose', 817: 'supposed', 818: 'sure', 819: 'surprise', 820: 'swear', 821: 'sweet', 822: 'sweetie', 823: 'swimming', 824: 'swine', 825: 'taken', 826: 'taking', 827: 'talent', 828: 'talk', 829: 'talking', 830: 'tan', 831: 'taste', 832: 'tea', 833: 'team', 834: 'teeth', 835: 'tell', 836: 'telling', 837: 'terrible', 838: 'test', 839: 'text', 840: 'tha', 841: 'thank', 842: 'thanks', 843: 'thats', 844: 'theres', 845: 'theyre', 846: 'thing', 847: 'think', 848: 'thinking', 849: 'tho', 850: 'thought', 851: 'throat', 852: 'throw', 853: 'thunder', 854: 'ticket', 855: 'til', 856: 'till', 857: 'time', 858: 'times', 859: 'tired', 860: 'today', 861: 'told', 862: 'tomorrow', 863: 'tonight', 864: 'tonite', 865: 'took', 866: 'totally', 867: 'touch', 868: 'tough', 869: 'tour', 870: 'town', 871: 'track', 872: 'traffic', 873: 'trailer', 874: 'train', 875: 'training', 876: 'travel', 877: 'trek', 878: 'tried', 879: 'trip', 880: 'trouble', 881: 'true', 882: 'truly', 883: 'trust', 884: 'try', 885: 'trying', 886: 'tummy', 887: 'turn', 888: 'turned', 889: 'turning', 890: 'turns', 891: 'tweet', 892: 'twice', 893: 'twilight', 894: 'twitter', 895: 'twittering', 896: 'type', 897: 'ugh', 898: 'ugly', 899: 'um', 900: 'understand', 901: 'unfortunately', 902: 'unless', 903: 'update', 904: 'upset', 905: 'ur', 906: 'use', 907: 'used', 908: 'usual', 909: 'usually', 910: 'vacation', 911: 'version', 912: 'video', 913: 'view', 914: 'visit', 915: 'voice', 916: 'vote', 917: 'wait', 918: 'waiting', 919: 'wake', 920: 'waking', 921: 'walk', 922: 'walking', 923: 'wall', 924: 'want', 925: 'wanting', 926: 'warm', 927: 'wash', 928: 'wasnt', 929: 'waste', 930: 'wat', 931: 'watch', 932: 'watched', 933: 'watching', 934: 'water', 935: 'way', 936: 'wear', 937: 'wearing', 938: 'weather', 939: 'web', 940: 'wedding', 941: 'week', 942: 'weekend', 943: 'weird', 944: 'welcome', 945: 'went', 946: 'werent', 947: 'west', 948: 'wet', 949: 'weve', 950: 'whats', 951: 'white', 952: 'wife', 953: 'win', 954: 'window', 955: 'wine', 956: 'wish', 957: 'wishing', 958: 'wit', 959: 'woke', 960: 'woman', 961: 'won', 962: 'wonder', 963: 'wonderful', 964: 'wondering', 965: 'wont', 966: 'woo', 967: 'woohoo', 968: 'word', 969: 'work', 970: 'worked', 971: 'working', 972: 'workout', 973: 'works', 974: 'world', 975: 'worried', 976: 'worry', 977: 'worse', 978: 'worst', 979: 'worth', 980: 'wouldnt', 981: 'wow', 982: 'write', 983: 'writing', 984: 'wrong', 985: 'wrote', 986: 'ya', 987: 'yea', 988: 'yeah', 989: 'year', 990: 'yep', 991: 'yes', 992: 'yesterday', 993: 'yo', 994: 'york', 995: 'youd', 996: 'young', 997: 'youve', 998: 'yr', 999: 'yummy'}\n",
      "nan\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "print(index_dict)\n",
    "print(data.iloc[4]['text3'])\n",
    "print(tf.toarray()[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(learning_method='online', learning_offset=50.0,\n",
       "                          max_iter=5, n_components=20, random_state=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: just got ready pretty party car stop enjoy pic ago\n",
      "Topic #1: need morning sick feeling wont havent girl baby leave family\n",
      "Topic #2: dont know sad wait rain start waiting eat stupid money\n",
      "Topic #3: twitter didnt nice people old talk mean picture heart concert\n",
      "Topic #4: want right bad did come cool wow job stay idea\n",
      "Topic #5: work think tonight nan doesnt look later god hour crazy\n",
      "Topic #6: going night days hey amazing ya end needs hit goes\n",
      "Topic #7: thanks tomorrow yeah bed bit play said lot check sweet\n",
      "Topic #8: today watching working tired watch let cold woke food room\n",
      "Topic #9: sleep getting little saw yesterday till hair use cute thinking\n",
      "Topic #10: happy great thank say maybe trying follow stuff id read\n",
      "Topic #11: hope sorry better soon live hear making music beautiful exam\n",
      "Topic #12: oh thats hate house early big hi cause following course\n",
      "Topic #13: make sure thing away thought hot weather funny outside place\n",
      "Topic #14: day time new week school today doing sun ugh hell\n",
      "Topic #15: home fun way long lost late far listening believe wasnt\n",
      "Topic #16: best life ur movie help finally coming does birthday song\n",
      "Topic #17: like feel man guess left tweet damn actually gone theres\n",
      "Topic #18: love really ill wish awesome weekend summer try year tell\n",
      "Topic #19: good miss yes phone went looking friend game glad world\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_words(lda, tf_feature_names, 10) #change n_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>text2</th>\n",
       "      <th>text3</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_remove_stopwords</th>\n",
       "      <th>length_1</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2059014670</td>\n",
       "      <td>Sat Jun 06 16:04:41 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>patriciya</td>\n",
       "      <td>@pearl sayang di tayo blockmates.  omg the cas...</td>\n",
       "      <td>sayang di tayo blockmates omg the cases are so...</td>\n",
       "      <td>di the are so</td>\n",
       "      <td>[]</td>\n",
       "      <td>['di', 'the', 'are', 'so']</td>\n",
       "      <td>['di']</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1548317796</td>\n",
       "      <td>Fri Apr 17 20:37:23 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>emma__watson</td>\n",
       "      <td>in... scotland  twitters may be sparse for a w...</td>\n",
       "      <td>in scotland twitters may be sparse for a while...</td>\n",
       "      <td>in may be sparse for a while but i intend to d...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['in', 'may', 'be', 'sparse', 'for', 'a', 'whi...</td>\n",
       "      <td>['may', 'sparse', 'intend', 'best']</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1998735976</td>\n",
       "      <td>Mon Jun 01 19:02:28 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>PinkTribble</td>\n",
       "      <td>why has the sciatic discomfort chosen to retur...</td>\n",
       "      <td>why has the sciatic discomfort chosen to retur...</td>\n",
       "      <td>why the sciatic discomfort chosen to return ju...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['why', 'the', 'sciatic', 'discomfort', 'chose...</td>\n",
       "      <td>['sciatic', 'discomfort', 'chosen', 'return', ...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2063144198</td>\n",
       "      <td>Sun Jun 07 01:31:48 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Raycast</td>\n",
       "      <td>@pembsdave lol, sounds like a good way to be! ...</td>\n",
       "      <td>lol sounds like a good way to be enjoy your no...</td>\n",
       "      <td>like a good way to be enjoy your no</td>\n",
       "      <td>[]</td>\n",
       "      <td>['like', 'a', 'good', 'way', 'to', 'be', 'enjo...</td>\n",
       "      <td>['like', 'good', 'way', 'enjoy']</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1992040864</td>\n",
       "      <td>Mon Jun 01 08:05:55 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>laurennicole514</td>\n",
       "      <td>about to go to work.</td>\n",
       "      <td>about to go to work</td>\n",
       "      <td>about to go to work</td>\n",
       "      <td>[]</td>\n",
       "      <td>['about', 'to', 'go', 'to', 'work']</td>\n",
       "      <td>['go', 'work']</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316640</th>\n",
       "      <td>0</td>\n",
       "      <td>1881534122</td>\n",
       "      <td>Fri May 22 04:55:25 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>gqgoat</td>\n",
       "      <td>the rain still persists here in gainesville, f...</td>\n",
       "      <td>the rain still persists here in gainesville fl...</td>\n",
       "      <td>the rain still here in hope not too many to ce...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['the', 'rain', 'still', 'here', 'in', 'hope',...</td>\n",
       "      <td>['rain', 'still', 'hope', 'many', 'central', '...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316641</th>\n",
       "      <td>0</td>\n",
       "      <td>2190311283</td>\n",
       "      <td>Tue Jun 16 02:21:51 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>starchild20</td>\n",
       "      <td>@stacijshelton thanks -i am looking for inspoi...</td>\n",
       "      <td>thanks i am looking for inspoiration and encou...</td>\n",
       "      <td>thanks i am looking for and encouragement at m...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['thanks', 'i', 'am', 'looking', 'for', 'and',...</td>\n",
       "      <td>['thanks', 'looking', 'encouragement', 'mo', '...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316642</th>\n",
       "      <td>4</td>\n",
       "      <td>2187459719</td>\n",
       "      <td>Mon Jun 15 20:18:48 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ryking</td>\n",
       "      <td>@rickydee55 i do have a reputation, don't i? s...</td>\n",
       "      <td>i do have a reputation dont i sorry</td>\n",
       "      <td>i do have a reputation dont i sorry</td>\n",
       "      <td>[]</td>\n",
       "      <td>['i', 'do', 'have', 'a', 'reputation', 'dont',...</td>\n",
       "      <td>['reputation', 'dont', 'sorry']</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316643</th>\n",
       "      <td>0</td>\n",
       "      <td>2175490828</td>\n",
       "      <td>Mon Jun 15 01:08:06 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Vivicam</td>\n",
       "      <td>emma you tool! and to think i had to walk home...</td>\n",
       "      <td>emma you tool and to think i had to walk home ...</td>\n",
       "      <td>emma you tool and to think i had to walk home ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['emma', 'you', 'tool', 'and', 'to', 'think', ...</td>\n",
       "      <td>['emma', 'tool', 'think', 'walk', 'home', 'dar...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316644</th>\n",
       "      <td>0</td>\n",
       "      <td>2003979499</td>\n",
       "      <td>Tue Jun 02 07:28:10 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>merel84</td>\n",
       "      <td>@mgardot i've been looking on the net but i ca...</td>\n",
       "      <td>ive been looking on the net but i cant find yo...</td>\n",
       "      <td>been looking on the net but i cant find your a...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['been', 'looking', 'on', 'the', 'net', 'but',...</td>\n",
       "      <td>['looking', 'net', 'cant', 'find', 'coming', '...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316645 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target          id                          date      flag  \\\n",
       "0            0  2059014670  Sat Jun 06 16:04:41 PDT 2009  NO_QUERY   \n",
       "1            4  1548317796  Fri Apr 17 20:37:23 PDT 2009  NO_QUERY   \n",
       "2            0  1998735976  Mon Jun 01 19:02:28 PDT 2009  NO_QUERY   \n",
       "3            4  2063144198  Sun Jun 07 01:31:48 PDT 2009  NO_QUERY   \n",
       "4            0  1992040864  Mon Jun 01 08:05:55 PDT 2009  NO_QUERY   \n",
       "...        ...         ...                           ...       ...   \n",
       "316640       0  1881534122  Fri May 22 04:55:25 PDT 2009  NO_QUERY   \n",
       "316641       0  2190311283  Tue Jun 16 02:21:51 PDT 2009  NO_QUERY   \n",
       "316642       4  2187459719  Mon Jun 15 20:18:48 PDT 2009  NO_QUERY   \n",
       "316643       0  2175490828  Mon Jun 15 01:08:06 PDT 2009  NO_QUERY   \n",
       "316644       0  2003979499  Tue Jun 02 07:28:10 PDT 2009  NO_QUERY   \n",
       "\n",
       "                   user                                               text  \\\n",
       "0             patriciya  @pearl sayang di tayo blockmates.  omg the cas...   \n",
       "1          emma__watson  in... scotland  twitters may be sparse for a w...   \n",
       "2           PinkTribble  why has the sciatic discomfort chosen to retur...   \n",
       "3               Raycast  @pembsdave lol, sounds like a good way to be! ...   \n",
       "4       laurennicole514                              about to go to work.    \n",
       "...                 ...                                                ...   \n",
       "316640           gqgoat  the rain still persists here in gainesville, f...   \n",
       "316641      starchild20  @stacijshelton thanks -i am looking for inspoi...   \n",
       "316642           ryking  @rickydee55 i do have a reputation, don't i? s...   \n",
       "316643          Vivicam  emma you tool! and to think i had to walk home...   \n",
       "316644          merel84  @mgardot i've been looking on the net but i ca...   \n",
       "\n",
       "                                                    text2  \\\n",
       "0       sayang di tayo blockmates omg the cases are so...   \n",
       "1       in scotland twitters may be sparse for a while...   \n",
       "2       why has the sciatic discomfort chosen to retur...   \n",
       "3       lol sounds like a good way to be enjoy your no...   \n",
       "4                                     about to go to work   \n",
       "...                                                   ...   \n",
       "316640  the rain still persists here in gainesville fl...   \n",
       "316641  thanks i am looking for inspoiration and encou...   \n",
       "316642                i do have a reputation dont i sorry   \n",
       "316643  emma you tool and to think i had to walk home ...   \n",
       "316644  ive been looking on the net but i cant find yo...   \n",
       "\n",
       "                                                    text3 hashtags  \\\n",
       "0                                           di the are so       []   \n",
       "1       in may be sparse for a while but i intend to d...       []   \n",
       "2       why the sciatic discomfort chosen to return ju...       []   \n",
       "3                     like a good way to be enjoy your no       []   \n",
       "4                                     about to go to work       []   \n",
       "...                                                   ...      ...   \n",
       "316640  the rain still here in hope not too many to ce...       []   \n",
       "316641  thanks i am looking for and encouragement at m...       []   \n",
       "316642                i do have a reputation dont i sorry       []   \n",
       "316643  emma you tool and to think i had to walk home ...       []   \n",
       "316644  been looking on the net but i cant find your a...       []   \n",
       "\n",
       "                                                   tokens  \\\n",
       "0                              ['di', 'the', 'are', 'so']   \n",
       "1       ['in', 'may', 'be', 'sparse', 'for', 'a', 'whi...   \n",
       "2       ['why', 'the', 'sciatic', 'discomfort', 'chose...   \n",
       "3       ['like', 'a', 'good', 'way', 'to', 'be', 'enjo...   \n",
       "4                     ['about', 'to', 'go', 'to', 'work']   \n",
       "...                                                   ...   \n",
       "316640  ['the', 'rain', 'still', 'here', 'in', 'hope',...   \n",
       "316641  ['thanks', 'i', 'am', 'looking', 'for', 'and',...   \n",
       "316642  ['i', 'do', 'have', 'a', 'reputation', 'dont',...   \n",
       "316643  ['emma', 'you', 'tool', 'and', 'to', 'think', ...   \n",
       "316644  ['been', 'looking', 'on', 'the', 'net', 'but',...   \n",
       "\n",
       "                                  tokens_remove_stopwords  length_1  \\\n",
       "0                                                  ['di']         1   \n",
       "1                     ['may', 'sparse', 'intend', 'best']         4   \n",
       "2       ['sciatic', 'discomfort', 'chosen', 'return', ...         8   \n",
       "3                        ['like', 'good', 'way', 'enjoy']         4   \n",
       "4                                          ['go', 'work']         2   \n",
       "...                                                   ...       ...   \n",
       "316640  ['rain', 'still', 'hope', 'many', 'central', '...         9   \n",
       "316641  ['thanks', 'looking', 'encouragement', 'mo', '...         5   \n",
       "316642                    ['reputation', 'dont', 'sorry']         3   \n",
       "316643  ['emma', 'tool', 'think', 'walk', 'home', 'dar...         9   \n",
       "316644  ['looking', 'net', 'cant', 'find', 'coming', '...        10   \n",
       "\n",
       "        num_hashtags  sentiment_score  \n",
       "0                  0              3.0  \n",
       "1                  0              3.0  \n",
       "2                  0              1.0  \n",
       "3                  0              4.0  \n",
       "4                  0              1.0  \n",
       "...              ...              ...  \n",
       "316640             0              2.0  \n",
       "316641             0              5.0  \n",
       "316642             0              3.0  \n",
       "316643             0              5.0  \n",
       "316644             0              4.0  \n",
       "\n",
       "[316645 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "aim = pd.read_csv(\"Tweets_sentiment.csv\")\n",
    "aim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>sentiment_score</th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>3.0</th>\n",
       "      <th>4.0</th>\n",
       "      <th>5.0</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62560</td>\n",
       "      <td>21484</td>\n",
       "      <td>31130</td>\n",
       "      <td>10036</td>\n",
       "      <td>33873</td>\n",
       "      <td>159083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23914</td>\n",
       "      <td>7128</td>\n",
       "      <td>26122</td>\n",
       "      <td>21683</td>\n",
       "      <td>78715</td>\n",
       "      <td>157562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>86474</td>\n",
       "      <td>28612</td>\n",
       "      <td>57252</td>\n",
       "      <td>31719</td>\n",
       "      <td>112588</td>\n",
       "      <td>316645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "sentiment_score    1.0    2.0    3.0    4.0     5.0     All\n",
       "target                                                     \n",
       "0                62560  21484  31130  10036   33873  159083\n",
       "4                23914   7128  26122  21683   78715  157562\n",
       "All              86474  28612  57252  31719  112588  316645"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(aim['target'], aim['sentiment_score'], margins=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
