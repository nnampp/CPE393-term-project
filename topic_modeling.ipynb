{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Topic Modeling**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('Tweets_cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.sample(frac = 0.005)\n",
    "# data = data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>text2</th>\n",
       "      <th>text3</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_remove_stopwords</th>\n",
       "      <th>length_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - awww, t...</td>\n",
       "      <td>awww thats a bummer you shoulda got david carr...</td>\n",
       "      <td>thats a bummer you got carr of third day to do...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['thats', 'a', 'bummer', 'you', 'got', 'carr',...</td>\n",
       "      <td>['thats', 'bummer', 'got', 'carr', 'third', 'd...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "      <td>is upset that he cant update his by it and mig...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['is', 'upset', 'that', 'he', 'cant', 'update'...</td>\n",
       "      <td>['upset', 'cant', 'update', 'might', 'cry', 'r...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@kenichan i dived many times for the ball. man...</td>\n",
       "      <td>i dived many times for the ball managed to sav...</td>\n",
       "      <td>i many times for the ball to save the rest go ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['i', 'many', 'times', 'for', 'the', 'ball', '...</td>\n",
       "      <td>['many', 'times', 'ball', 'save', 'rest', 'go']</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body itchy and like its on fire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['my', 'whole', 'body', 'itchy', 'and', 'like'...</td>\n",
       "      <td>['whole', 'body', 'itchy', 'like', 'fire']</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>no its not behaving at all im mad why am i her...</td>\n",
       "      <td>no its not at all mad why am i here because i ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['no', 'its', 'not', 'at', 'all', 'mad', 'why'...</td>\n",
       "      <td>['mad', 'cant', 'see']</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag   \n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  \\\n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text   \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - awww, t...  \\\n",
       "1    scotthamilton  is upset that he can't update his facebook by ...   \n",
       "2         mattycus  @kenichan i dived many times for the ball. man...   \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                               text2   \n",
       "0  awww thats a bummer you shoulda got david carr...  \\\n",
       "1  is upset that he cant update his facebook by t...   \n",
       "2  i dived many times for the ball managed to sav...   \n",
       "3     my whole body feels itchy and like its on fire   \n",
       "4  no its not behaving at all im mad why am i her...   \n",
       "\n",
       "                                               text3 hashtags   \n",
       "0  thats a bummer you got carr of third day to do...      NaN  \\\n",
       "1  is upset that he cant update his by it and mig...      NaN   \n",
       "2  i many times for the ball to save the rest go ...      NaN   \n",
       "3           my whole body itchy and like its on fire      NaN   \n",
       "4  no its not at all mad why am i here because i ...      NaN   \n",
       "\n",
       "                                              tokens   \n",
       "0  ['thats', 'a', 'bummer', 'you', 'got', 'carr',...  \\\n",
       "1  ['is', 'upset', 'that', 'he', 'cant', 'update'...   \n",
       "2  ['i', 'many', 'times', 'for', 'the', 'ball', '...   \n",
       "3  ['my', 'whole', 'body', 'itchy', 'and', 'like'...   \n",
       "4  ['no', 'its', 'not', 'at', 'all', 'mad', 'why'...   \n",
       "\n",
       "                             tokens_remove_stopwords  length_1  \n",
       "0  ['thats', 'bummer', 'got', 'carr', 'third', 'd...         6  \n",
       "1  ['upset', 'cant', 'update', 'might', 'cry', 'r...        10  \n",
       "2    ['many', 'times', 'ball', 'save', 'rest', 'go']         6  \n",
       "3         ['whole', 'body', 'itchy', 'like', 'fire']         5  \n",
       "4                             ['mad', 'cant', 'see']         3  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LDA (Latent Dirichlet Allocation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "\n",
    "# parameter setting\n",
    "n_samples = data.shape[0]\n",
    "n_features = 3000 # number of feature words\n",
    "n_components = 0 # number of topic (just declare 0 at first)\n",
    "n_top_words = 15 # number of top words in each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(max_df=0.99, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(data['text3'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1600000x3000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6238720 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_samples=1600000, n_features=3000 n_components=5 \n",
      "sklearn log-likelihood: -45006498.639\n",
      "sklearn perplexity: 1153.582\n",
      "Fitting LDA models with tf features, n_samples=1600000, n_features=3000 n_components=6 \n",
      "sklearn log-likelihood: -45280786.244\n",
      "sklearn perplexity: 1204.231\n",
      "Fitting LDA models with tf features, n_samples=1600000, n_features=3000 n_components=7 \n",
      "sklearn log-likelihood: -45471528.747\n",
      "sklearn perplexity: 1240.758\n",
      "Fitting LDA models with tf features, n_samples=1600000, n_features=3000 n_components=8 \n",
      "sklearn log-likelihood: -45625955.398\n",
      "sklearn perplexity: 1271.140\n",
      "Fitting LDA models with tf features, n_samples=1600000, n_features=3000 n_components=9 \n",
      "sklearn log-likelihood: -45842020.809\n",
      "sklearn perplexity: 1314.903\n",
      "Fitting LDA models with tf features, n_samples=1600000, n_features=3000 n_components=10 \n",
      "sklearn log-likelihood: -45944664.717\n",
      "sklearn perplexity: 1336.217\n"
     ]
    }
   ],
   "source": [
    "# Find the best n_components by perplexity to specify number of topic used for training model\n",
    "\n",
    "for i in range(5,11,1):    \n",
    "    n_topics = i\n",
    "\n",
    "    print(\"Fitting LDA models with tf features, \"\n",
    "          \"n_samples=%d, n_features=%d n_components=%d \"\n",
    "          % (n_samples, n_features, n_topics))\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, max_iter=5,\n",
    "                                    learning_method='online',\n",
    "                                    learning_offset=50.,\n",
    "                                    random_state=0)\n",
    "    lda.fit(tf)\n",
    "\n",
    "    likelihood = lda.score(tf)\n",
    "    perplexity = lda.perplexity(tf)\n",
    "\n",
    "    if n_topics == 5 or (n_topics != 1 and perplexity < best_perplexity):\n",
    "        best_n_components = i\n",
    "        best_perplexity = perplexity\n",
    "\n",
    "    print('sklearn log-likelihood: %.3f' % likelihood)\n",
    "    print('sklearn perplexity: %.3f' % perplexity)\n",
    "\n",
    "n_components = best_n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(learning_method='online', learning_offset=50.0,\n",
       "                          max_iter=5, n_components=5, random_state=0)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % (topic_idx+1)\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1: good know day got morning really just hope happy dont night thank think thats make\n",
      "Topic #2: thanks ill watching just hey did yes tonight best doing today like song ya later\n",
      "Topic #3: going nice want awesome day say week need sure twitter look pretty birthday looking working\n",
      "Topic #4: just new fun home nan time days amazing glad movie like way check let old\n",
      "Topic #5: love today work wait come like oh getting day right feel weekend life follow got\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_interpret = lda.transform(tf)\n",
    "lda_label = []\n",
    "\n",
    "for i in range(len(lda_interpret)):\n",
    "    lda_label.append(np.argmax(lda_interpret[i])+1)\n",
    "\n",
    "data['topic_LDA'] = lda_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic_LDA\n",
       "1    456386\n",
       "5    317759\n",
       "2    278496\n",
       "4    276308\n",
       "3    271051\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['topic_LDA'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.839412</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.040175</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.040413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.028614</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.885672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.050615</td>\n",
       "      <td>0.797972</td>\n",
       "      <td>0.050004</td>\n",
       "      <td>0.050521</td>\n",
       "      <td>0.050888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.598170</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.101830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>0.033546</td>\n",
       "      <td>0.235421</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.331033</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>0.299210</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.050790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>0.639055</td>\n",
       "      <td>0.040171</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.040491</td>\n",
       "      <td>0.040283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          topic_1   topic_2   topic_3   topic_4   topic_5\n",
       "0        0.839412  0.040000  0.040175  0.040000  0.040413\n",
       "1        0.028571  0.028614  0.028571  0.028571  0.885672\n",
       "2        0.840000  0.040000  0.040000  0.040000  0.040000\n",
       "3        0.050615  0.797972  0.050004  0.050521  0.050888\n",
       "4        0.598170  0.100000  0.100000  0.100000  0.101830\n",
       "...           ...       ...       ...       ...       ...\n",
       "1599995  0.033546  0.235421  0.200000  0.331033  0.200000\n",
       "1599996  0.299210  0.300000  0.050000  0.300000  0.050790\n",
       "1599997  0.066667  0.400000  0.066667  0.066667  0.400000\n",
       "1599998  0.639055  0.040171  0.240000  0.040491  0.040283\n",
       "1599999  0.600000  0.100000  0.100000  0.100000  0.100000\n",
       "\n",
       "[1600000 rows x 5 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_doc_by_topic = pd.DataFrame(lda_interpret, columns=[(\"topic_%d\"%(i+1)) for i in range(n_components)]) \n",
    "lda_doc_by_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandoned</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accent</th>\n",
       "      <th>accept</th>\n",
       "      <th>accepted</th>\n",
       "      <th>access</th>\n",
       "      <th>accident</th>\n",
       "      <th>...</th>\n",
       "      <th>youve</th>\n",
       "      <th>yr</th>\n",
       "      <th>yuck</th>\n",
       "      <th>yucky</th>\n",
       "      <th>yummy</th>\n",
       "      <th>zac</th>\n",
       "      <th>zero</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83.498625</td>\n",
       "      <td>0.201753</td>\n",
       "      <td>0.202986</td>\n",
       "      <td>0.203731</td>\n",
       "      <td>0.202488</td>\n",
       "      <td>0.202365</td>\n",
       "      <td>0.202414</td>\n",
       "      <td>0.202345</td>\n",
       "      <td>0.201912</td>\n",
       "      <td>0.201294</td>\n",
       "      <td>...</td>\n",
       "      <td>3307.861134</td>\n",
       "      <td>0.202571</td>\n",
       "      <td>0.202038</td>\n",
       "      <td>0.203097</td>\n",
       "      <td>0.202444</td>\n",
       "      <td>0.202628</td>\n",
       "      <td>0.202643</td>\n",
       "      <td>0.203497</td>\n",
       "      <td>412.092462</td>\n",
       "      <td>0.202679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.202408</td>\n",
       "      <td>169.692192</td>\n",
       "      <td>0.203134</td>\n",
       "      <td>0.203386</td>\n",
       "      <td>0.202549</td>\n",
       "      <td>0.202703</td>\n",
       "      <td>0.203393</td>\n",
       "      <td>0.202488</td>\n",
       "      <td>0.202664</td>\n",
       "      <td>294.040748</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203230</td>\n",
       "      <td>1185.157795</td>\n",
       "      <td>0.201064</td>\n",
       "      <td>0.202817</td>\n",
       "      <td>3477.319710</td>\n",
       "      <td>0.203605</td>\n",
       "      <td>342.282684</td>\n",
       "      <td>0.203640</td>\n",
       "      <td>0.203897</td>\n",
       "      <td>0.202266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.204182</td>\n",
       "      <td>0.202200</td>\n",
       "      <td>3492.663758</td>\n",
       "      <td>0.203616</td>\n",
       "      <td>0.202630</td>\n",
       "      <td>0.203528</td>\n",
       "      <td>0.203084</td>\n",
       "      <td>0.203436</td>\n",
       "      <td>542.217395</td>\n",
       "      <td>0.201681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203341</td>\n",
       "      <td>0.203033</td>\n",
       "      <td>0.202043</td>\n",
       "      <td>89.121477</td>\n",
       "      <td>0.203202</td>\n",
       "      <td>0.202706</td>\n",
       "      <td>0.202703</td>\n",
       "      <td>0.203184</td>\n",
       "      <td>0.203971</td>\n",
       "      <td>0.203823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.203797</td>\n",
       "      <td>0.203255</td>\n",
       "      <td>0.202823</td>\n",
       "      <td>276.472816</td>\n",
       "      <td>0.202980</td>\n",
       "      <td>0.202434</td>\n",
       "      <td>359.818404</td>\n",
       "      <td>0.202688</td>\n",
       "      <td>0.203302</td>\n",
       "      <td>0.201951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203165</td>\n",
       "      <td>0.203756</td>\n",
       "      <td>201.292657</td>\n",
       "      <td>0.203457</td>\n",
       "      <td>0.203415</td>\n",
       "      <td>309.302671</td>\n",
       "      <td>0.203309</td>\n",
       "      <td>0.204757</td>\n",
       "      <td>0.205091</td>\n",
       "      <td>717.931632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.202770</td>\n",
       "      <td>0.201865</td>\n",
       "      <td>0.202493</td>\n",
       "      <td>0.203129</td>\n",
       "      <td>2663.414785</td>\n",
       "      <td>413.234064</td>\n",
       "      <td>0.203362</td>\n",
       "      <td>322.350511</td>\n",
       "      <td>0.202624</td>\n",
       "      <td>0.201581</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203004</td>\n",
       "      <td>0.203114</td>\n",
       "      <td>0.202517</td>\n",
       "      <td>0.202484</td>\n",
       "      <td>0.202732</td>\n",
       "      <td>0.203425</td>\n",
       "      <td>0.201612</td>\n",
       "      <td>273.289137</td>\n",
       "      <td>0.204189</td>\n",
       "      <td>0.203712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abandoned     ability         able    absolute   absolutely      accent   \n",
       "0  83.498625    0.201753     0.202986    0.203731     0.202488    0.202365  \\\n",
       "1   0.202408  169.692192     0.203134    0.203386     0.202549    0.202703   \n",
       "2   0.204182    0.202200  3492.663758    0.203616     0.202630    0.203528   \n",
       "3   0.203797    0.203255     0.202823  276.472816     0.202980    0.202434   \n",
       "4   0.202770    0.201865     0.202493    0.203129  2663.414785  413.234064   \n",
       "\n",
       "       accept    accepted      access    accident  ...        youve   \n",
       "0    0.202414    0.202345    0.201912    0.201294  ...  3307.861134  \\\n",
       "1    0.203393    0.202488    0.202664  294.040748  ...     0.203230   \n",
       "2    0.203084    0.203436  542.217395    0.201681  ...     0.203341   \n",
       "3  359.818404    0.202688    0.203302    0.201951  ...     0.203165   \n",
       "4    0.203362  322.350511    0.202624    0.201581  ...     0.203004   \n",
       "\n",
       "            yr        yuck      yucky        yummy         zac        zero   \n",
       "0     0.202571    0.202038   0.203097     0.202444    0.202628    0.202643  \\\n",
       "1  1185.157795    0.201064   0.202817  3477.319710    0.203605  342.282684   \n",
       "2     0.203033    0.202043  89.121477     0.203202    0.202706    0.202703   \n",
       "3     0.203756  201.292657   0.203457     0.203415  309.302671    0.203309   \n",
       "4     0.203114    0.202517   0.202484     0.202732    0.203425    0.201612   \n",
       "\n",
       "       zombie        zone         zoo  \n",
       "0    0.203497  412.092462    0.202679  \n",
       "1    0.203640    0.203897    0.202266  \n",
       "2    0.203184    0.203971    0.203823  \n",
       "3    0.204757    0.205091  717.931632  \n",
       "4  273.289137    0.204189    0.203712  \n",
       "\n",
       "[5 rows x 3000 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_topic_by_word = pd.DataFrame(lda.components_, columns=tf_feature_names)\n",
    "lda_topic_by_word"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GSDMM (Gibbs Sampling Dirichlet Multinomial Mixture)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import gsdmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "docs = []\n",
    "for i in range(len(data)):\n",
    "    docs.append(ast.literal_eval(data['tokens_remove_stopwords'].values[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 1271746 clusters with 5 clusters populated\n",
      "In stage 1: transferred 1236730 clusters with 5 clusters populated\n",
      "In stage 2: transferred 1192846 clusters with 5 clusters populated\n",
      "In stage 3: transferred 1074449 clusters with 5 clusters populated\n",
      "In stage 4: transferred 880702 clusters with 5 clusters populated\n"
     ]
    }
   ],
   "source": [
    "mgp = gsdmm.MovieGroupProcess(K=n_components, alpha=0.1, beta=0.5, n_iters=5)\n",
    "vocab = set(x for doc in docs for x in doc)\n",
    "n_terms = len(vocab)\n",
    "y = mgp.fit(docs, n_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words(cluster_word_distribution, top_cluster, values):\n",
    "    for cluster in top_cluster:\n",
    "        sort_dicts =sorted(mgp.cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
    "        print('Topic %s: %s'%(cluster+1,sort_dicts))\n",
    "\n",
    "def topic_allocation(df, docs, mgp):\n",
    "    topic_allocations = []\n",
    "    for doc in tqdm(docs):\n",
    "        topic_label, score = mgp.choose_best_label(doc)\n",
    "        topic_allocations.append(topic_label+1)\n",
    "    df['topic_GSDMM'] = topic_allocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('like', 21891), ('dont', 17361), ('get', 16207), ('got', 14772), ('one', 12838), ('really', 11180), ('know', 10951), ('think', 10692), ('still', 9989), ('today', 9773), ('want', 9315), ('cant', 9293), ('feel', 9100), ('need', 8929), ('go', 8501)]\n",
      "Topic 2: [('like', 11444), ('good', 10608), ('get', 9235), ('one', 8537), ('got', 8153), ('love', 7849), ('new', 7460), ('go', 7317), ('time', 7192), ('today', 7020), ('going', 6738), ('cant', 6696), ('day', 6513), ('want', 6360), ('dont', 5811)]\n",
      "Topic 3: [('day', 50886), ('good', 36357), ('work', 35974), ('today', 34807), ('go', 33372), ('going', 32114), ('back', 26168), ('night', 25916), ('home', 24391), ('morning', 23167), ('time', 22209), ('get', 22086), ('tomorrow', 21109), ('got', 19870), ('last', 18954)]\n",
      "Topic 4: [('love', 24358), ('cant', 15499), ('like', 15058), ('new', 15042), ('see', 13949), ('u', 13806), ('good', 12635), ('dont', 10859), ('one', 10317), ('go', 10150), ('know', 9941), ('watching', 9485), ('really', 9224), ('got', 9103), ('oh', 8627)]\n",
      "Topic 5: [('u', 33291), ('get', 25382), ('dont', 24382), ('know', 23521), ('good', 21389), ('thanks', 20262), ('love', 18847), ('twitter', 17549), ('cant', 16780), ('see', 15811), ('like', 15791), ('well', 14841), ('day', 14758), ('ill', 13701), ('go', 13572)]\n"
     ]
    }
   ],
   "source": [
    "doc_count = np.array(mgp.cluster_doc_count)\n",
    "topic_indices = np.arange(start=0, stop=len(doc_count), step=1)\n",
    "top_words(mgp.cluster_word_distribution, topic_indices, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600000/1600000 [02:16<00:00, 11736.55it/s]\n"
     ]
    }
   ],
   "source": [
    "topic_allocation(data, docs, mgp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic_GSDMM\n",
       "3    513577\n",
       "5    413121\n",
       "1    257087\n",
       "4    244021\n",
       "2    172194\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['topic_GSDMM'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600000/1600000 [02:07<00:00, 12560.48it/s]\n"
     ]
    }
   ],
   "source": [
    "doc_score = []\n",
    "\n",
    "for doc in tqdm(docs):\n",
    "    score = mgp.score(doc)\n",
    "    doc_score.append(score)\n",
    "\n",
    "doc_score = np.array(doc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.044519</td>\n",
       "      <td>0.083944</td>\n",
       "      <td>0.100030</td>\n",
       "      <td>0.735720</td>\n",
       "      <td>0.035788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.756307</td>\n",
       "      <td>0.008895</td>\n",
       "      <td>0.145259</td>\n",
       "      <td>0.005365</td>\n",
       "      <td>0.084175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.333088</td>\n",
       "      <td>0.105224</td>\n",
       "      <td>0.316931</td>\n",
       "      <td>0.112683</td>\n",
       "      <td>0.132074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.874997</td>\n",
       "      <td>0.039213</td>\n",
       "      <td>0.075351</td>\n",
       "      <td>0.010362</td>\n",
       "      <td>0.000077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.053134</td>\n",
       "      <td>0.031021</td>\n",
       "      <td>0.094107</td>\n",
       "      <td>0.481101</td>\n",
       "      <td>0.340637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>0.010311</td>\n",
       "      <td>0.002492</td>\n",
       "      <td>0.966059</td>\n",
       "      <td>0.017642</td>\n",
       "      <td>0.003495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>0.066799</td>\n",
       "      <td>0.032209</td>\n",
       "      <td>0.008024</td>\n",
       "      <td>0.281482</td>\n",
       "      <td>0.611486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>0.042322</td>\n",
       "      <td>0.419802</td>\n",
       "      <td>0.294648</td>\n",
       "      <td>0.212386</td>\n",
       "      <td>0.030842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>0.002469</td>\n",
       "      <td>0.008111</td>\n",
       "      <td>0.802928</td>\n",
       "      <td>0.074360</td>\n",
       "      <td>0.112132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>0.054689</td>\n",
       "      <td>0.069106</td>\n",
       "      <td>0.400436</td>\n",
       "      <td>0.183849</td>\n",
       "      <td>0.291920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          topic_1   topic_2   topic_3   topic_4   topic_5\n",
       "0        0.044519  0.083944  0.100030  0.735720  0.035788\n",
       "1        0.756307  0.008895  0.145259  0.005365  0.084175\n",
       "2        0.333088  0.105224  0.316931  0.112683  0.132074\n",
       "3        0.874997  0.039213  0.075351  0.010362  0.000077\n",
       "4        0.053134  0.031021  0.094107  0.481101  0.340637\n",
       "...           ...       ...       ...       ...       ...\n",
       "1599995  0.010311  0.002492  0.966059  0.017642  0.003495\n",
       "1599996  0.066799  0.032209  0.008024  0.281482  0.611486\n",
       "1599997  0.042322  0.419802  0.294648  0.212386  0.030842\n",
       "1599998  0.002469  0.008111  0.802928  0.074360  0.112132\n",
       "1599999  0.054689  0.069106  0.400436  0.183849  0.291920\n",
       "\n",
       "[1600000 rows x 5 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsdmm_doc_by_topic = pd.DataFrame(doc_score, columns=[(\"topic_%d\"%(i+1)) for i in range(n_components)]) \n",
    "gsdmm_doc_by_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_importance(mgp):\n",
    "    n_z_w = mgp.cluster_word_distribution\n",
    "    beta, V, K = mgp.beta, mgp.vocab_size, mgp.K\n",
    "    phi = [{} for i in range(K)]\n",
    "    for z in range(K):\n",
    "        for w in n_z_w[z]:\n",
    "            phi[z][w] = (n_z_w[z][w]+beta)/(sum(n_z_w[z].values())+V*beta)\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = cluster_importance(mgp)\n",
    "topic_vocab = []\n",
    "vocab_list = list(vocab)\n",
    "\n",
    "for i in range(n_components):\n",
    "    temp = []\n",
    "    for j in range(len(vocab_list)):\n",
    "        try:\n",
    "            temp.append(phi[i][vocab_list[j]])\n",
    "        except:\n",
    "            temp.append(0)\n",
    "    topic_vocab.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>siss</th>\n",
       "      <th>reaver</th>\n",
       "      <th>autonomic</th>\n",
       "      <th>metatarsal</th>\n",
       "      <th>lagger</th>\n",
       "      <th>agger</th>\n",
       "      <th>fit</th>\n",
       "      <th>toggle</th>\n",
       "      <th>township</th>\n",
       "      <th>clearance</th>\n",
       "      <th>...</th>\n",
       "      <th>thirty</th>\n",
       "      <th>matchbox</th>\n",
       "      <th>unstraightened</th>\n",
       "      <th>china</th>\n",
       "      <th>peel</th>\n",
       "      <th>balancer</th>\n",
       "      <th>saga</th>\n",
       "      <th>calorific</th>\n",
       "      <th>eastbound</th>\n",
       "      <th>philander</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.970169e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000416</td>\n",
       "      <td>1.661695e-06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>9.970169e-07</td>\n",
       "      <td>2.326373e-06</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.326373e-06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.157935e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>3.593592e-06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.996440e-06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.058178e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.054521e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>2.586658e-06</td>\n",
       "      <td>7.054521e-07</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.054521e-07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.107802e-05</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>1.786777e-06</td>\n",
       "      <td>1.072066e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>3.216199e-06</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.515899e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.113612e-07</td>\n",
       "      <td>8.113612e-07</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>8.113612e-07</td>\n",
       "      <td>8.113612e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.113612e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           siss    reaver     autonomic    metatarsal        lagger     agger   \n",
       "0  9.970169e-07  0.000000  0.000000e+00  0.000000e+00  0.000000e+00  0.000000  \\\n",
       "1  1.157935e-05  0.000000  0.000000e+00  0.000000e+00  0.000000e+00  0.000001   \n",
       "2  1.058178e-05  0.000000  0.000000e+00  0.000000e+00  7.054521e-07  0.000000   \n",
       "3  1.107802e-05  0.000001  0.000000e+00  0.000000e+00  0.000000e+00  0.000000   \n",
       "4  3.515899e-06  0.000000  8.113612e-07  8.113612e-07  0.000000e+00  0.000000   \n",
       "\n",
       "        fit        toggle      township  clearance  ...    thirty   \n",
       "0  0.000416  1.661695e-06  0.000000e+00   0.000003  ...  0.000022  \\\n",
       "1  0.000271  0.000000e+00  0.000000e+00   0.000024  ...  0.000011   \n",
       "2  0.000089  0.000000e+00  0.000000e+00   0.000001  ...  0.000031   \n",
       "3  0.000146  1.786777e-06  1.072066e-06   0.000000  ...  0.000006   \n",
       "4  0.000097  8.113612e-07  8.113612e-07   0.000000  ...  0.000002   \n",
       "\n",
       "       matchbox  unstraightened     china      peel  balancer      saga   \n",
       "0  9.970169e-07    2.326373e-06  0.000018  0.000026  0.000002  0.000003  \\\n",
       "1  3.593592e-06    0.000000e+00  0.000134  0.000020  0.000000  0.000007   \n",
       "2  2.586658e-06    7.054521e-07  0.000039  0.000011  0.000000  0.000002   \n",
       "3  3.216199e-06    0.000000e+00  0.000036  0.000003  0.000000  0.000074   \n",
       "4  0.000000e+00    0.000000e+00  0.000102  0.000000  0.000000  0.000000   \n",
       "\n",
       "   calorific     eastbound     philander  \n",
       "0   0.000000  2.326373e-06  0.000000e+00  \n",
       "1   0.000002  1.996440e-06  0.000000e+00  \n",
       "2   0.000000  7.054521e-07  0.000000e+00  \n",
       "3   0.000000  0.000000e+00  0.000000e+00  \n",
       "4   0.000000  0.000000e+00  8.113612e-07  \n",
       "\n",
       "[5 rows x 31216 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsdmm_topic_by_word = pd.DataFrame(topic_vocab, columns=vocab_list) \n",
    "gsdmm_topic_by_word"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with topic label (LDA, GSDMM)\n",
    "data.to_csv('Tweets_topic_modeling.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA document by topic \n",
    "# probability of each document relate to each topic\n",
    "lda_doc_by_topic.to_csv('LDA_doc_by_topic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA topic by word\n",
    "# importance of the word to each topic\n",
    "lda_topic_by_word.to_csv('LDA_topic_by_word.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GSDMM document by topic \n",
    "# probability of each document relate to each topic\n",
    "gsdmm_doc_by_topic.to_csv('GSDMM_doc_by_topic.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GSDMM topic by word\n",
    "# importance of the word to each topic\n",
    "gsdmm_topic_by_word.to_csv('GSDMM_topic_by_word.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
